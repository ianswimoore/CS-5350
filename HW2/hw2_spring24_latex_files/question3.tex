\section{Mistake Bound Model of Learning}\label{sec:q3}

In both the questions below, we will consider functions defined over
$n$ Boolean features. That is, each example in our learning problem
is a $n$-dimensional vector from $\{0, 1\}^n$. We will use the
symbol $\bx$ to denote an example and $\bx_i$ denotes its $i^{th}$
element.  (We will assume that there is no noise involved.)

For all questions below, it is not enough to just state the
answer. You need to justify your answer with a short proof.


\begin{enumerate}
\item Consider the concept class $\mathcal{C}_1$ defined as follows:
  Each element of $\mathcal{C}_1$ is defined using a fixed instance
  $\bz \in \{0, 1\}^n$ as follows:
  \begin{equation*}
    f_\bz(\bx) = \begin{cases}
      1 & \bx = \bz \\
      0 & \bx \ne \bz.
    \end{cases}
  \end{equation*}
  That is, the function $f_\bz$ predicts $1$ if, and only if, the
  input to the function is $\bz$.

  Our goal is to come up with a mistake bound algorithm that will
  learn any function $f\in\mathcal{C}_1$.

  \begin{enumerate}
  \item~[5 points] Determine $\vert\mathcal{C}_1\vert$, the size of
    concept class.

  {\color{red}
    The size of the concept class $\mathcal{C}_1$ is $2^n$.\\
    \textbf{Proof:} For each of the $n$ features, we have two choices: $0$ or $1$. Therefore, the total number of functions in the concept class is $2^n$.
  }

  \item~[15 points] Write a mistake bound learning algorithm for
    this concept class that makes no more than {\em one} mistake on
    any sequence of examples presented to it. Please write the
    algorithm concisely in the form of pseudocode.

    Prove the mistake bound for this algorithm.

  {\color{red}
    Goal: Learn$f \in C_1$ with at most one mistake. \\
    The algorithm will work as follows:

    \begin{algorithm}[H]
      \caption{Mistake Bound Learning Algorithm for $\mathcal{C}_1$}
      \begin{algorithmic}[1]
        \State Initialize $h(\bx) = 0$ (always predict $0$)
        \For each example $\bx$ presented to the algorithm
        \If {$h$ predicts correctly}
        \State continue
        \EndIf
        \If {$h$ predicts incorrectly}
        \State update $h$ to predict $1$ if the example is $\bx$
        \EndIf
        \EndFor
      \end{algorithmic}
    \end{algorithm}

    \textbf{Proof of Mistake Bound:} The algorithm will make at most one mistake. If the algorithm makes a mistake, it will update the hypothesis to predict $1$ for the example that was misclassified. After this update, the algorithm will not make any more mistakes since there is exactly one element $\bx$ that exactly equals $z$. Therefore, the algorithm will make at most one mistake.
  }

  \end{enumerate}

\item Suppose we have a concept class $\mathcal{C}_2$ that consists
  of exactly $n$ functions $\{f_1, f_2, \cdots, f_n\}$, where each
  function $f_i$ is defined as follows:

  \begin{equation*}
    f_i(\bx) = \bx_i.
  \end{equation*}
  That is, the function $f_i$ returns the value of the $i^{th}$
  feature.

  \begin{enumerate}
  \item~[5 points] How many mistakes will the algorithm
    \textbf{CON} from class make on any function from this concept
    class?

    {\color{red}
      In general, CON will make at most $|C|-1$ mistakes on any concept class $C$.\\

    }

  \item~[5 points] How many mistakes will the Halving algorithm make
    on any function from this concept class?
  \end{enumerate}

\end{enumerate}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: "hw2"
%%% End:
